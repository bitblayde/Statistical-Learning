---
title: "Hoja 3 Aprendizaje Estadístico"
author: "Pablo Manresa Nebot"
date: "10 de enero de 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<h3 style="color:red">
Ejercicio 1: (4.8.6 ISL) página 191.
</h3>

6. Suppose we collect data for a group of students in a statistics class with variables X1=hours studied, X2=undergrad GPA, and Y=receive an A. We fit a logistic regression and produce estimated coefficient, $\hat{β}_0=−6, \hat{β}_1=0.05, \hat{β}_2= 1$.

(a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.

<h4 style="color:blue">  Solución </h4>
\

$$ p(\hat{x}) = \frac{e^{\hat{β}_0+\hat{β}_1X_1 + \hat{β}_2 X_2}}{1+e^{\hat{β}_0+\hat{β}_1X_1 + \hat{β}_2 X_2}}$$
Sustituyendo:

$$ p(\hat{x}) = \frac{e^{-6+0.05 · 40 + 1 · 3.5}}{1+e^{-6+0.05 · 40 + 1 · 3.5}} = 0.377 $$
\
\
(b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?

<h4 style="color:blue">  Solución </h4>
\

$$ p(\hat{x}) = \frac{e^{-6+0.05X_1 + 1 · 3.5 }}{1+e^{-6+0.05X_1 + 1 · 3.5 }} = 0.5$$
Equivalentemente:

$$e^{-6+0.05X_1 + 1 · 3.5 } = 1 $$ 

tomando logaritmos para eliminar la exponencial:

$$ -6+0.05X_1 + 1 · 3.5 = ln(1) $$

$$X_1 = \frac{ln(1)+2.5}{0.05}$$
$$X_1 = 50$$
Un estudiante con un GPA de 3.5 necesitaría estudiar 50 horas para tener un 50% de posibilidades de obtener un A.


\
\
\
\

<h3 style="color:red">
Ejercicio 2: (4.8.8 ISL) página 191-192.
</h3>

8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures
First we use logistic regression and get an error rate of 20 % on the training data and 30 % on the test data. Next we use 1-nearest neighbors (i.e. K= 1) and get an average error rate (averaged over bothtest and training data sets) of 18 %. Based on these results, which method should we prefer to use for classification of new observations? Why?

<h4 style="color:blue">  Solución </h4>
\

Se tiene que el modelo de regresión logística presenta una tasa de error del 30% en los datos de test. Por otro lado, el KNN con K=1, presenta un error medio entre train y test del 18%. ¿Cuál método es preferible para clasificar nuevas observaciones? Sería preferible aquel método que presente una menor tasa de error en el conjunto de test. Por un lado se sabe que la tasa de error en test con el ajuste de regresión logística es del 30%. Sin embargo, dado que el ajuste KNN, usa un K=1, se ajusta a cada dato de entrenamiento, produciendo un error en train de 0, por tanto, el error de test real, sería del 36% para el modelo de KNN con K=1. Por todo lo anterior, preferiría el ajuste con regresión logística.

\
\
\
\

<h3 style="color:red">
Ejercicio 3: (4.8.14 ISL) página 194 (a), (b), (c), (f), (h).
</h3>

14.In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.


(a) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.

<h4 style="color:blue">  Solución </h4>

Como mencionó el profesor en mensaje, una vez se cree la variable mpg01, la variable mpg se eliminará.

```{r, message=FALSE}
library("ISLR2")
attach(Auto)

# Se calcula la mediana
mpg.median <- median(Auto$mpg)

mpg.median

# Se guarda un 1 si el valor es mayor a su mediana, si no, se almacena un 0
mpg01 <- ifelse(Auto$mpg>mpg.median, 1, 0)

# Se crea el dataframe con todas las variables excepto mpg
auto.data.frame <- data.frame(Auto[, -1], mpg01)

head(auto.data.frame, 5)
```

(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatter plots and boxplots may be useful tools to answer this question. Describe your findings.

<h4 style="color:blue">  Solución </h4>

Para ver la asociación se podría, o bien comprobar la matriz de correlación, o visualmente graficando dicha variable en función de las otras. En este caso, se usará la matriz de correlación, dado que es lo más directo. Además, posteriormente se graficarán.

```{r}
# Se crea la matriz de correlación
cor(auto.data.frame[, -8])
```

En cuanto a los boxplot:

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 1. Gráficos de caja dataset Auto."}
# Se crean los gráficos de caja
boxplot(auto.data.frame$cylinders~auto.data.frame$mpg01, main="Cilindros y mpg01")
```

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 2. Gráficos de caja dataset Auto."}
boxplot(auto.data.frame$displacement~auto.data.frame$mpg01, main="Desplazamiento y mpg01")
```

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 3. Gráficos de caja dataset Auto."}
boxplot(auto.data.frame$horsepower~auto.data.frame$mpg01, main="Caballos (potencia) y mpg01")
```

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 4. Gráficos de caja dataset Auto."}
boxplot(auto.data.frame$weight~auto.data.frame$mpg01, main="Peso y mpg01")
```

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 5. Gráficos de caja dataset Auto."}
boxplot(auto.data.frame$acceleration~auto.data.frame$mpg01, main="Aceleración y mpg01")
```

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 6. Gráficos de caja dataset Auto."}
boxplot(auto.data.frame$year~auto.data.frame$mpg01, main="Año y mpg01")
```

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 7. Gráficos de caja dataset Auto."}
boxplot(auto.data.frame$origin~auto.data.frame$mpg01, main="Origen y mpg01")
```
\

Existe relación entre mpg01 y cilindros, desplazamiento, caballos (potencia) y peso.

(c) Split the data into a training set and a test set.

<h4 style="color:blue">  Solución </h4>

Se dividen los datos a la mitad, 60% para entrenamiento, y 40% para test.

```{r}
set.seed(12)

# Se generan los índices de train
train.indices <- sample.int(n=nrow(auto.data.frame), size=round(nrow(auto.data.frame)*0.6) )
# Se divide entre train y test
train <- auto.data.frame[train.indices, ]
test <- auto.data.frame[-train.indices, ]

# Se guarda el vector con la variable respuesta para el conjunto de test
respuesta.test <- auto.data.frame[-train.indices, "mpg01"]
```

(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?

<h4 style="color:blue">  Solución </h4>

```{r, message=FALSE, warning=FALSE}
# Se realiza el ajuste con las variables que más correlacionadas dado el apartado b
ajuste.reg.logistica <- glm(mpg01 ~ displacement+cylinders+horsepower+weight, data=train, family=binomial)

summary(ajuste.reg.logistica)
```

Se realiza el ajuste con regresión logística:

```{r}
tabla <- matrix(NA, ncol=2, nrow=5)

# Se calculan las predicciones
predicciones.reg.log <- predict(ajuste.reg.logistica, test, type="response")

# Si es mayor a 0.5, entonces pertenece a la clase 1
resultados <- ifelse(predicciones.reg.log>0.5, 1, 0)

# Se muestra la matriz de confusión
table(resultados, respuesta.test)
```
Para test se han acertado 135 observaciones, y se ha errado en 22 observaciones.

```{r}
tabla[1, 1] <- mean(resultados == respuesta.test)
tabla[1, 2] <- mean(resultados != respuesta.test)

mean(resultados == respuesta.test)

mean(resultados != respuesta.test)
```
En cuanto al porcentaje del error en test, se ha obtenido un error en el conjunto de test del 14%, frente a un acierto del 85.98% 

(h) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?

<h4 style="color:blue">  Solución </h4>

Para esto, se usará como referencia la página 182 del *ISL*.

```{r}
library(class)

# Se prepara el formato adecuado el algoritmo KNN
train.x <- cbind(displacement, cylinders, horsepower, weight)[train.indices, ]
test.x <- cbind(displacement, cylinders, horsepower, weight)[-train.indices, ]

train.mpg01 <- auto.data.frame[train.indices, "mpg01"]
```

Para k = 1:

```{r}
k = 1
# Se realiza el ajuste con k = 1
knn.pred.k1 <- knn(train.x, test.x, train.mpg01, k=k)

table(knn.pred.k1, respuesta.test)

tabla[2, 1] <- mean(knn.pred.k1 == respuesta.test)
tabla[2, 2] <- mean(knn.pred.k1 != respuesta.test)

# Se calcula el error y el acierto
mean(knn.pred.k1==respuesta.test)
mean(knn.pred.k1!=respuesta.test)
```

Con k=5:

```{r}
k = 5

# Se realiza el ajuste con k = 5 vecinos
knn.pred.k5 <- knn(train.x, test.x, train.mpg01, k=k)

table(knn.pred.k5, respuesta.test)

tabla[3, 1] <- mean(knn.pred.k5 == respuesta.test)
tabla[3, 2] <- mean(knn.pred.k5 != respuesta.test)

# Se calcula el error y el acierto
mean(knn.pred.k5==respuesta.test)
mean(knn.pred.k5!=respuesta.test)
```

Con k=10:

```{r}
k = 10

# Se realiza el ajuste con k = 10 vecinos
knn.pred.k10 <- knn(train.x, test.x, train.mpg01, k=k)

table(knn.pred.k10, respuesta.test)

tabla[4, 1] <- mean(knn.pred.k10 == respuesta.test)
tabla[4, 2] <- mean(knn.pred.k10 != respuesta.test)

# Se calcula el error y el acierto
mean(knn.pred.k10==respuesta.test)
mean(knn.pred.k10!=respuesta.test)
```

Con k=50:

```{r}
k = 50

# Se realiza el ajuste con k = 50 vecinos
knn.pred.k50 <- knn(train.x, test.x, train.mpg01, k=k)

table(knn.pred.k50, respuesta.test)

tabla[5, 1] <- mean(knn.pred.k50 == respuesta.test)
tabla[5, 2] <- mean(knn.pred.k50 != respuesta.test)

# Se calcula el error y el acierto
mean(knn.pred.k50==respuesta.test)
mean(knn.pred.k50!=respuesta.test)
```

```{r}
# Se crea la tabla
rownames(tabla) <- c("regresión logística", "KNN k=1", "KNN k=5", "KNN k=10", "KNN k=50")
colnames(tabla) <- c("tasa acierto", "tasa error")
knitr::kable(round(tabla, digits=4), caption="Tabla 1: Error en test con el conjunto de datos Auto")
```
Para los diferentes valores de k, donde menor tasa de error se obtiene en test, es para el ajuste con k = 10 vecinos, obteniendo una tasa de acierto del 87.26%.




\
\
\
\

<h3 style="color:red">
Ejercicio 4.
</h3>

4. A partir del conjunto de datos de entrenamiento y del conjunto de datos de prueba utilizados en el ejercicio anterior:

(a) Aplica  SVM  con  el  kernel  lineal  con  diferentes  valores  del  parámetro cost. Utiliza  la validación  cruzada  para  determinar  el  valor   óptimo  del  parámetro cost.   Escribe  el clasificador  SVM  asociado  al  valor óptimo  del  parámetro cost y  calcula  el  error  de clasificación en el conjunto test.

<h4 style="color:blue">  Solución </h4>

Como este ejercicio indica que hemos de usar los mismos datos que en el anterior, se usarán entonces, los mismos predictores.

Se usará un k = 10.

```{r, message=FALSE, warning=FALSE}
library(e1071)
library(caret)

tabla.ej4 = matrix(ncol=2, nrow=4)

data.train=data.frame(displacement=train$displacement, cylinders=train$cylinders, horsepower=train$horsepower, weight=train$weight, mpg01=as.factor(train$mpg01))
data.test = data.frame(displacement=test$displacement, cylinders=test$cylinders, horsepower=test$horsepower, weight=test$weight, mpg01=as.factor(test$mpg01))


n=nrow(data.train)
# Vector con el parámetro costest
Cost.vec=c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)
filas=length(Cost.vec)
# número de particiones
kfolds=10
Accuracy.CV=matrix(rep(NA,filas*2),ncol=2)
set.seed(4)
folds=sample(rep(1:kfolds,length=n))
 
for (c in 1:filas){
cv.accuracy=rep(NA, kfolds)
for ( k in 1:kfolds){
    # Se realiza el ajuste lineal
    model.cv.temp=svm(mpg01~., data=data.train[folds!=k, ], kernel="linear", cost=Cost.vec[c],scale=FALSE)
    # Se realiza la predicción
    pred=predict(model.cv.temp, data.train[folds==k,])
    # Se calcula la exactitud
    cv.accuracy[k]=mean(pred==data.train[folds==k,]$mpg01)
 }
Accuracy.CV[c,]=c(mean( cv.accuracy),sd(cv.accuracy))
}
RESULTS.CV=cbind(Cost.vec,Accuracy.CV)
colnames(RESULTS.CV)=c("Coste (C)","Tasa de acierto", "Sd")
RESULTS.CV
```

Se obtiene le mejor parámetro:

```{r}
# Se obtiene el mejor C de entre todos los probados
best.cost.parameter=Cost.vec[Accuracy.CV[,1]==max(Accuracy.CV[,1])][1]
best.cost.parameter
```

La gráfica con el acierto en función del valor del parámetro coste:

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 8. Tasa de acierto en función del valor coste."}
index.best.Cost.CV=seq(1,filas)[Cost.vec==best.cost.parameter][1]
plot(seq(1,filas),Accuracy.CV[,1],type="b", xaxt='n', ylab="Tasa de acierto (10-CV)",xlab="Coste (C)")
axis(side = 1, at = seq(1,filas), labels = as.character(Cost.vec), tck = -0.01)
points(index.best.Cost.CV,Accuracy.CV[index.best.Cost.CV,1], type="p",col="red",pch=19,cex=1.3)
axis(side = 1, at = index.best.Cost.CV, labels = as.character(Cost.vec[index.best.Cost.CV]), tck = -0.01,lwd.ticks=3,col.ticks="red")
legend("right",inset=0.03, legend=c("Max Tasa de acierto"),col="red",pch=19)
```



El mejor clasificador de acuerdo a la validación cruzada y el valor de coste:

```{r}
svmfit.best=svm(mpg01~., data=data.train, kernel="linear", cost=best.cost.parameter,scale=FALSE)
```

Los coeficientes:

```{r}
beta = drop(t(svmfit.best$coefs)%*%as.matrix(data.train[svmfit.best$index,-5]))
beta
``` 

```{r}
beta0 = -svmfit.best$rho
beta0
```

Se tiene w = (-0,0066727, -0,5548598, -0,0112279, -0,0002216), b = 5,66586 con C = 0.1:

El clasificador óptimo $$c(dis, cyl, horsep, weight) = \left\{ \begin{array}{ll}
        clase \, 1 & \mbox{si $  -0.0066727 x_{1}  -0.5548598 x_{2}  -0.0112279 x_{3} -0.0002216 x_{4} + 5.66586 \geq 0$}\\
        & \\
       clase \, 0 & \mbox{si $ -0.0066727 x_{1}  -0.5548598 x_{2}  -0.0112279 x_{3} -0.0002216 x_{4} + 5.66586 < 0$}
        .\end{array} \right. $$

El acierto en train:

```{r}
# Predicción con train
y.pred.lineal = predict(svmfit.best, data.train)

table(y.pred.lineal, data.train$mpg01)
mean(y.pred.lineal==data.train$mpg01)
```

El acierto en test:

```{r}
# Predicción con test
y.pred.lineal = predict(svmfit.best, data.test)

tabla.ej4[3, 1] = round(mean(y.pred.lineal==data.test$mpg01), digits=2)
tabla.ej4[3, 2] = round(mean(y.pred.lineal!=data.test$mpg01), digits=2)

table(y.pred.lineal, data.test$mpg01)
mean(y.pred.lineal==data.test$mpg01)
```

Los valores $\ge 0 $ representan la clase 1, y aquellos < 0 representan la clase 0.

```{r}
y.pred.aux = predict(svmfit.best, data.train, decision.values = TRUE)
y.pred.aux
```


(b) Aplica SVM con el kernel radial con diferentes valores del parámetro cost y del parámetro gamma del kernel radial.  Utiliza la validación cruzada para determinar el valor óptimo de los párametros y calcula el error de clasificación en el conjunto test.

<h4 style="color:blue">  Solución </h4>

```{r}
set.seed(2)

n=nrow(data.train)

# Parámetros a los que aplicar el tune
Cost.vec=c(0.001, 0.01, 0.1, 1, 5, 10)
Gamma.vec=c(0.005,0.01,0.1,1,2,3,4)
matriz.parametros=expand.grid(Cost.vec,Gamma.vec)

filas=nrow(matriz.parametros)

# Número de particiones
k.folds=10
aciertos.cv.radial = matrix(rep(NA,filas*2),ncol=2)

# Se crean las particiones
particiones=sample(rep(1:k.folds,length=n))

for (i in 1:filas){
  cv.accuracy=rep(NA, k.folds)
  
  for ( k in 1: k.folds){
      # Se realiza el ajuste con la partición k
      svm.vc=svm(mpg01~., data=data.train[particiones!=k, ], kernel="radial", cost=matriz.parametros[i, 1], gamma=matriz.parametros[i, 2], scale=FALSE) 
      pred=predict(svm.vc, data.train[particiones==k,])
      # Se calcula el acierto
      cv.accuracy[k]=mean(pred==data.train[particiones==k,]$mpg01)
        
  }
aciertos.cv.radial[i,]=c(mean(cv.accuracy),sd(cv.accuracy))
}

svm.cv.radial.resultados=cbind(matriz.parametros,aciertos.cv.radial)
colnames(svm.cv.radial.resultados)=c("Coste (C)", "gamma", "Tasa de acierto","Sd")
```

Los resultados:

```{r}
svm.cv.radial.resultados
```

Los mejores parámetros son los siguientes:

```{r}
best.parameters=matriz.parametros[aciertos.cv.radial[,1]==max(aciertos.cv.radial[,1]),][1,]
best.parameters
```

Se crea el clasficador con los mejores parámetros según la validación cruzada:

```{r, warning=FALSE, message=FALSE}
svmfit.best=svm(mpg01~., data=data.train, kernel="radial", gamma=best.parameters[2], cost=best.parameters[1], scale=FALSE)
```


Las clases:

```{r}
y.pred.rad = predict(svmfit.best, data.train, decision.values=TRUE)
y.pred.rad
```

La clase 1 es aquella cuyos valores son $\ge 0$, es decir, el sign(f) es positivo, y la clase 0, serían aquellos < 0. Por ejemplo, la observación 1 con el valor 1,1039431 se asigna a la clase 1, y el 6, con el valor -1,0000845 se asigna a la clase 0.

Se tiene un acierto en train:

```{r}
# Predicción con train
y.pred.radial.train = predict(svmfit.best, data.train)

table(y.pred.radial.train, data.train$mpg01)
mean(y.pred.radial.train==data.train$mpg01)
```

Y un acierto en test:

```{r}
# Predicción con test
y.pred.radial.test = predict(svmfit.best, data.test)

tabla.ej4[4, 1] = round(mean(y.pred.radial.test==data.test$mpg01), digits=4)
tabla.ej4[4, 2] = round(mean(y.pred.radial.test!=data.test$mpg01), digits=4)

table(y.pred.radial.test, data.test$mpg01)
mean(y.pred.radial.test==data.test$mpg01)
mean(y.pred.radial.test!=data.test$mpg01)
```

En test se tiene un error del 17.3%.


(c) Teniendo en cuenta los resultados anteriores y los resultados de los apartados (f) y (h) del ejercicio 3 ¿Cuál de los cuatro clasificadores utilizarías para realizar predicciones?

<h4 style="color:blue">  Solución </h4>

```{r}
tabla.ej4[1, 1] = round(tabla[1, 1], digits=4)
tabla.ej4[1, 2] = round(tabla[1, 2], digits=4)


tabla.ej4[2, ] = tabla[which.max(tabla[1:nrow(tabla), ] ), ]

mejor.k <- rownames(tabla)[which.max(tabla[1:nrow(tabla), ] )]

rownames(tabla.ej4) <- c("regresión logística", mejor.k, "SVM-lineal", "SVM-radial")
colnames(tabla.ej4) <- c("tasa acierto", "tasa error")
knitr::kable(tabla.ej4, caption="Tabla 2: Error en test con el conjunto de datos Auto")
```

Como puede observarse, con el modelo SVM lineal se tiene una tasa de acierto del 90%, la cual es superior a los otros tres, por tanto, elegiría dicho clasificador. En segundo lugar se tiene el KNN con k = 10, que presenta una tasa de acierto del 87.26%, que se encuentra ligeramente por encima de la tasa de acierto de la regresión logística, la cual presenta una tasa de acierto del 86%. En último lugar se tiene el SVM con kernel radial (no lineal) con un 79% de tasa de acierto. Parece que los modelos lineales producen una mejor aproximación que el modelo no lineal.

\
\
\
\

<h3 style="color:red">
Ejercicio 5: (9.7.2 ISL) página 398.
</h3>

2.We have seen that in p= 2 dimensions, a linear decision boundary takes the form $β_0+β_1X_1+β_2 X_2 = 0$. We now investigate a non-linear decision boundary.

(a) Sketch the curve 

$(1 +X_1)^2+ (2−X_2)^2=4$.

<h4 style="color:blue">  Solución </h4>

Lo anterior se corresponde con la ecuación de la circunferencia, es decir:

$(x-a)^2 + (y-b)^2 = r^2$

Por tanto, se corresponde a una circunferencia con x = -1, y = 2, y r = 2.

```{r, fig.width=10, fig.height=8, fig.cap="Figura 9. $(1 +X_1)^2+ (2−X_2)^2=4$."}
library(plotrix)

plot(NA, NA, type="n", xlim = c(-5, 3), ylim = c(-2, 6), asp=1, xlab="X_1", ylab="X_2")
draw.circle(-1, 2, 2)
```


(b) On your sketch, indicate the set of points for which

$(1 +X_1)^2+ (2−X_2)^2>4$,

as well as the set of points for which

$(1 +X_1)^2+ (2−X_2)^2≤4$.

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=10, fig.height=8, fig.cap="Figura 10. $(1 +X_1)^2+ (2−X_2)^2>4$ y $(1 +X_1)^2+ (2−X_2)^2≤4$."}
plot(NA, NA, type="n", xlim = c(-5, 3), ylim = c(-2, 6), asp=1, xlab="X_1", ylab="X_2")
draw.circle(-1, 2, 2)
text(-1.5, 2, "<= 4")
text(4, 2, "> 4")
```

En el caso del interior de la circunferencia se tendría en cuenta también el borde, por eso sería $\le 4$.

(c) Suppose that a classifier assigns an observation to the blue class if

$(1 +X_1)^2+ (2−X_2)^2>4$,

and to the red class otherwise. To what class is the observation (0,0) classified? (−1,1)? (2,2)? (3,8)?

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=10, fig.height=8, fig.cap="Figura 11. Puntos de acuerdo a su clasificación."}
f <- function(x1, x2) (1+x1)^2+(2-x2)^2

obs1 <- f(0, 0)
obs2 <- f(-1, 1)
obs3 <- f(2, 2)
obs4 <- f(3, 8)

colors <- c("blue", "red", "blue", "blue")

plot(c(0, -1, 2, 3), c(0, 1, 2, 8), col=colors, type="p", xlim = c(-5, 3), ylim = c(-1, 9), asp=1, xlab="X_1", ylab="X_2")
draw.circle(-1, 2, 2)
text(-1.5, 2, "< 4")
text(4, 2, "> 4")

```

Las observarción (-1, 1) es clasificada en la clase roja ya que 1 es menor a 4, mientras que (0, 0), (2, 2) y (3, 8) son clasificadas en la clase azul, cuyos resultados son 5, 9, y 54 respectivamente.

(d) Argue that while the decision boundary in (c) is not linear interms of $X_1$ and $X_ 2$, it is linear in terms of $X1$, $X^2_1$, $X_2$, and $X^2_2$.

<h4 style="color:blue">  Solución </h4>

Esto podría comprobarse expandiendo $(1 +X_1)^2+ (2−X_2)^2=4$:

$1+X_1^2 + 2X_1 + 4 + X^2_2 - 4X_2 - 4 = 0$

$1 + 2X_1 + 4X_2 + X_1^2 + X^2_2 = 0$

Por lo que se tiene que es lineal en términos de $X1$, $X^2_1$, $X_2$, and $X^2_2$.

\
\
\
\

<h3 style="color:red">
Ejercicio 6: (9.7.5 ISL) página 399-400.
</h3>

5. We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features

(a) Generate a data set with n=500 and p=2, such that the obser-vations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows:

<h4 style="color:blue">  Solución </h4>

```{r}
set.seed(12)
x1<-runif(500) - 0.5
x2<-runif(500) - 0.5
y<-1*(x1^2-x2^2 > 0)

entrenamiento = data.frame(x1=x1, x2=x2, y=y)
```

(b) Plot the observations, colored according to their class labels. Your plot should display X1 on the x-axis, and X2 on the y-axis.

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=10, fig.height=8, fig.cap="Figura 12. apartado (b)"}
tipo.punto <- 2-y
color <- 3-y
plot(entrenamiento$x1, entrenamiento$x2, col=color, pch=tipo.punto, xlab="x_1", ylab="x_2")
```

(c) Fit a logistic regression model to the data, using X1 and X2 as predictors.

<h4 style="color:blue">  Solución </h4>

```{r, message=FALSE, warning=FALSE}
ajuste5c <- glm(y ~ x1+x2, data=entrenamiento, family=binomial)
summary(ajuste5c)
```

(d) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=10, fig.height=8, fig.cap="Figura 13. apartado (d)"}
reg.log.pred <- predict(ajuste5c, entrenamiento, type="response")

resultados <- ifelse(reg.log.pred>0.5, 1, 0)


plot(entrenamiento[resultados==1,]$x1, entrenamiento[resultados==1,]$x2, pch=1, col=2)
points(entrenamiento[resultados==0,]$x1, entrenamiento[resultados==0,]$x2, pch=2, col=3)
```

(e) Now fit a logistic regression model to the data using non-linear functions of $X_1$ and $X_2$ as predictors (e.g. $X^2_1$,$X1×X2$, $log(X_2)$,and so forth).

<h4 style="color:blue">  Solución </h4>

```{r, warning=FALSE, message=FALSE}
ajuste.e <- glm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2), data = entrenamiento, family=binomial)
summary(ajuste.e)
```
Todas las variables parecen ser estadísticamente significantes de acuerdo a su p-valor.

(f) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)-(e) until you come up with an example in which the predicted class labels are obviously non-linear.

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=10, fig.height=8, fig.cap="Figura 14. apartado (f)"}
reg.log.pred <- predict(ajuste.e, entrenamiento, type="response")

resultados <- ifelse(reg.log.pred>0.5, 1, 0)

plot(entrenamiento[resultados==1,]$x1, entrenamiento[resultados==1,]$x2, pch=1, col=2)
points(entrenamiento[resultados==0,]$x1, entrenamiento[resultados==0,]$x2, pch=2, col=3)
```
\
Como puede observarse, la frontera de decisión es no lineal. Además, presenta una alta similitud con la gráfica de (b).


(g) Fit a support vector classifier to the data with X1 and X2 as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=10, fig.height=8, fig.cap="Figura 15. apartado (g)"}
library(e1071)
entrenamiento.svm <- data.frame(x1=x1, x2=x2, y=as.factor(y))
ajuste.svm.g <- svm(y ~ x1+x2, data=entrenamiento.svm, kernel="linear", cost=1)

ypred <- predict(ajuste.svm.g, entrenamiento.svm)

table(predict=ypred, truth=entrenamiento.svm$y)

plot(entrenamiento[ypred==1,]$x1, entrenamiento[ypred==1,]$x2, pch=1, col=2)
points(entrenamiento[ypred==0,]$x1, entrenamiento[ypred==0,]$x2, pch=2, col=3)
```
El SVM con el núcleo lineal a penas clasifica elementos en la clase 0.

(h) Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.

<h4 style="color:blue">  Solución </h4>

Con un kernel polinómico de grado 2:

```{r, fig.width=10, fig.height=8, fig.cap="Figura 16. apartado (h) kernel polinómico"}
ajuste.svm.h.poly <- svm(y ~ x1+x2, data=entrenamiento.svm, kernel="polynomial", cost=1, degree=2)

summary(ajuste.svm.h.poly)
ypred <- predict(ajuste.svm.h.poly, entrenamiento.svm)

table(predict=ypred, truth=entrenamiento.svm$y)

plot(entrenamiento[ypred==1,]$x1, entrenamiento[ypred==1,]$x2, pch=1, col=2)
points(entrenamiento[ypred==0,]$x1, entrenamiento[ypred==0,]$x2, pch=2, col=3)
```

Con un kernel radial con $\gamma = 10$:

```{r, fig.width=10, fig.height=8, fig.cap="Figura 17. apartado (h) kernel radial"}
ajuste.svm.h.rad <- svm(y ~ x1+x2, data=entrenamiento.svm, kernel="radial", gamma=10)

summary(ajuste.svm.h.rad)
ypred <- predict(ajuste.svm.h.rad, entrenamiento.svm)

table(predict=ypred, truth=entrenamiento.svm$y)

plot(entrenamiento[ypred==1,]$x1, entrenamiento[ypred==1,]$x2, pch=1, col=2)
points(entrenamiento[ypred==0,]$x1, entrenamiento[ypred==0,]$x2, pch=2, col=3)
```

(i) Comment on your results.

Puede observarse en el apartado d, como la regresión logística usando únicamente $x_1$ y $x_2$ no se termina de ajustar a los datos, al igual que el svm con núcleo lineal. Sin embargo, la regresión logística con términos cuadráticos y de interacción presenta unas fronteras visualmente muy similares a los ajustes con svm no lineales (polinómico con grado 2 y radial con $\gamma=1$) y con los datos originales.

\
\
\
\

<h3 style="color:red">
Ejercicio 7.
</h3>

7.El conjunto de datos denominado USJudgeRatingscontiene la valoración que 43 jueces estatales del Tribunal Superior de EE.UU han obtenido de los abogados con los que trabajan, sobre aspectos medidos a través de 12 variables que puedes consultar en la ayuda: ?USJudgeRatings. Se pretende resumir la información disponible aplicando un Análisis de Componentes Principales con 2 ejes.

(a) ¿Qué varianza explica cada una de las componentes seleccionadas?

<h4 style="color:blue">  Solución </h4>

Para ver las opciones que brinda USJudgeRatings.:

```{r}
?USJudgeRatings
```

Antes de utilizar el algoritmo de Análisis de Componentes Principales, resultaría interesante conocer las correlaciones que hay entre variables del propio dataset, para así poder entender mejor los resultados del algoritmo.

```{r, message=FALSE, fig.width=10, fig.height=8, fig.cap="Figura 18. matriz de correlación."}
library("corrplot")
data(USJudgeRatings)
corrplot(cor(USJudgeRatings), method="number", type="upper")
```
Ahora, se utilizaría el algoritmo PCA sobre el dataset USJudgeRatings con el fin de obtener los componentes principales del mismo, y de esta forma visualizar qué porcentaje de la varianza explica cada uno de los componentes:

```{r, message=FALSE, warning=FALSE}
acp1 = prcomp(USJudgeRatings, scale=TRUE)

# varianza explicada por cada componente
Var.expl=acp1$sdev^2 

# proporción de varianza explicada por cada componente
PVE=100*Var.expl/sum(Var.expl) 

# proporción de varianza explicada acumulada
APVE=cumsum(PVE) 

# Matriz con PVE, APVE 
Var.Exp.mat=cbind(Var.expl,PVE,APVE )
colnames(Var.Exp.mat)=c("Varianza explicada", "% Varianza explicada", "% acumulado de varianza explicada")
knitr::kable(Var.Exp.mat, caption="Tabla 3.  Varianza explicada por las componentes principales", digits=2)
```
Como cabría esperar, el primer componente es el que mayor cantidad de varianza explica, en torno a 10.13 unidades, siendo un 84.45% del total, seguido del segundo componente, que explica 1.10 unidades de la varianza (en torno al 9.20%), luego el tercero que explica 0.33 unidades (un 2.77%) y finalmente el resto de componentes cuya explicación de la varianza es relativamente epqueña comparada a los dos primeros. El gráfico anterior de la matriz (triángulo superior) de correlación ha señalado indicios de que esto ocurriría dado que, como mostraba, existe una alta correlación lineal entre las variables (excepto la variable CONT), por tanto, no resulta extraño, que un único componente explique casi el 85% de la varianza.


(b) A partir del screeplot, ¿cuántas componentes habrías escogido?

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=10, fig.height=8  ,fig.cap="Figura 19. screeplot."}
par(mfrow=c(1,2))
plot(Var.expl,xlab="Componente principal",ylab="Varianza explicada",type="b",main="(a)",xaxt='n')
axis(side=1,at=1:12, labels = c(paste("PC", 1:12) ))
plot(cumsum(PVE),xlab="Componente principal",ylab="% acumulado de varianza explicada",ylim=c(40,100),type="b",main="(b)",xaxt='n')
axis(side=1,at=1:12, labels = c(paste("PC", 1:12)))
```
Sería más preciso e interesante usar el criterio del umbral de la proporción de la varianza explicada, donde se establezca un porcentaje de varianza a explicar. Esta es la regla más usada en la práctica, y el total suele rondar el 90%-95%. Por otro lado, se podría fijar un umbral para la varianza explicada por cada componente y establecerlo, en este dataset en concreto, por ejemplo, a 0.5. También sería interesante usar la regla de Kaiser que establece que que aquellos componentes cuyos autovalores son menores a 1, pueden ser no tenidos en cuenta. Y finalmente, estaría la "relga del codo", con la que, una vez ordenados los valores propios, se escogen hasta llegar a uno cuya no inclusión no suponga una gran pérdida de información. Como el enunciado menciona que cuantos componentes escogería a partir del screeplot, se hará uso de esta última y se escogerán los primeros dos componentes, dado que al no escoger el tercero, la pérdida de información es mínima, y entre los dos primeros componentes, se explica el 93.65% de la varianza.

(c) Interpreta el significado de las nuevas variables.

<h4 style="color:blue">  Solución </h4>

```{r}
scores.mat=acp1$x

Newscores.mat=-scores.mat

Interpretation.mat=cor(USJudgeRatings, Newscores.mat)
knitr::kable(Interpretation.mat, caption="Tabla 4a. Correlación entre las componentes principales y las variables originales .",digits=2)
```
El primer vector de carga (loading vector) asigna aproximadamente el mismo peso a todas las variables excepto la variable CONT, a la cual asigna mucho menor peso. Podría entederse que lo que captura este componente son las capacidades del abogado, en cuanto a su integridad, habilidades orales, diligencia, etc. en general, aquello que lo convierte en un abogado competente en base a sus propias capacidades, y aquellas variables que son consecuencia directa de las capacidades propias del abogado. Por ejemplo, un abogado diligente y competente, con altas capacidades orales, y una conducta ejemplar, va a tener de media, mayor flujo de casos, dado que será recomendado. Análogamente, un abogado con una conducta reprochable, poca diligencia, y malas capacidades orales, por lo general, tendrá un menor flujo de casos, menor retención con los clientes, etc. por tanto, existe una correlación. Por otro lado, el segundo componente, sin embargo, corresponde a la valoración en base al número de contactos que los abogados tienen con jueces. Esto tiene sentido, ya que, un buen abogado en base a sus propias capacidades debería ser independiente del número de contactos que tenga con jueces.

```{r, fig.width=20, fig.height=8  ,fig.cap="Figura 20. Correlación vs. componentes principales"}
par(mfrow=c(1,2))
barplot(Interpretation.mat[,1],main="Correlación de las variables originales con PC1",horiz = FALSE,ylim=c(0,1))
barplot(Interpretation.mat[,2],main="Correlación de las variables originales con PC2",horiz = FALSE,ylim=c(-1,1))
```
Además, aquí puede observarse como en la gráfica del componente 1, todas las variables excepto CONT (el número de contactos que los abogados tienen con jueces) están altamente correlacionadas de forma positiva, entendiendo así, que este componente sería la valoración del abogado en base a sus propias capacidades, por tanto, si presenta un valor alto en PC1 significa que tiene una buena valoración en sus capacidades.

Por otro lado, en el gráfico del segundo componente, éste tiene una correlación positiva alta con aquellos abogados con un alto número de contactos con jueces (CONT). Indicando que abogados con un valor alto en este componente, tienen un gran número de contactos con jueces (e incluso se podría entender, que cierto grado de influencia).

(d) Según la interpretación obtenida en el apartado anterior y observando el biplot, comenta cómo han sido valorados CALLAHAN y MIGNONE.

<h4 style="color:blue">  Solución </h4>

```{r , fig.width=14, fig.height=8  ,fig.cap="Figura 21 biplot."}

# Se crea la matriz de loadings
NewLoadings.mat =-acp1$rotation

s=summary(acp1)

plot(Newscores.mat[,1], Newscores.mat[,2], xlab=paste("PCA 1 (", round(s$importance[2]*100, 1), "%)", sep = ""), ylab=paste("PCA 2 (", round(s$importance[5]*100, 1), "%)", sep = ""),cex=1)       
# Add grid lines
abline(v=0, lty=2, col="grey50")
abline(h=0, lty=2, col="grey50")
# Add labels
text(Newscores.mat[,1], Newscores.mat[,2], labels=row.names(Newscores.mat), pos=c(1,3,4,2), font=2,cex=0.6)
# Get co-ordinates of variables (loadings), and multiply by 3
l.x <- NewLoadings.mat[,1]*3
l.y <- NewLoadings.mat[,2]*3
arrows(x0=0, x1=l.x, y0=0, y1=l.y, col="red", length=0.15, lwd=1.5)
l.pos <- l.y # Create a vector of y axis coordinates
lo <- which(l.y < 0) # Get the variables on the bottom half of the plot
hi <- which(l.y > 0) # Get variables on the top half
# Replace values in the vector
l.pos <- replace(l.pos, lo, "1")
l.pos <- replace(l.pos, hi, "3")
text(l.x, l.y, labels=row.names(NewLoadings.mat), col="red", pos=l.pos)
```
En cuanto a Callahan, R. J. (esquina superior derecha), tiene una valoración positiva en el PC1 (valor alto positivo), lo cual indica que tiene una valoración positiva de sus habilidades propias como abogado (diligencia, conducta, capacidades orales, etc.), además, también presenta un alto valor en el componente PC2, señalando que tiene una buena valoración en cuanto al número de contactos con jueces.

En cuanto al Mignone, A. F. (esquina inferior izquierda), presenta un valor bajo (negativo) en el PC1, lo cual indica que tiene **mala valoración** de sus capacidades propias como abogado, a diferencia, por ejemplo, de Callahan. Además, también presenta un valor negativo en el PC2, lo cual indica, que tiene una mala valoración en cuanto al número de contactos con jueces.

\
\
\
\

<h3 style="color:red">
Ejercicio 8: (12.6.4 ISL) página 559.
</h3>

4. Suppose that for a particular data set, we perform hierarchical clustering using single linkage and using complete linkage. We obtain two dendrograms.

(a) At a certain point on the single linkage dendrogram, the clusters {1,2,3} and {4,5} fuse. On the complete linkage dendrogram, the clusters {1,2,3} and {4,5} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?

<h4 style="color:blue">  Solución </h4>

Antes habría que repasar el significado de enlace (linkage) completo, el cual calcula todas las diferencias por pares entre las observaciones en el clúster A, y las observaciones en el clúster B, y registra la mayor de las diferencias. Por otro lado, el enlace (linkage) único (single), registra la más pequeña de las diferencias. Con ello en mente, dado la unión de dos clústers {1, 2, 3} y {4, 5} en cierto momento, no hay información suficiente como para saber si se fusionaron en la misma altura o no. Se sabe que si la diferencia mínima por ejemplo es 1, mientras que la máxima es 5, en el caso del enlace único, se fusionarían en la altura 1, mientras que en el enlace completo, se fusionarían en la altura 5. Por otra parte, si la diferencia por pares, es la misma en todos los casos, por ejemplo, 1, entonces, tanto con el enlace único como completo, se fusionarían en la altura 1.

(b) At a certain point on the single linkage dendrogram, the clusters {5} and {6} fuse. On the complete linkage dendrogram, the clusters {5} and {6} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?

<h4 style="color:blue">  Solución </h4>

Se fusionarían a la misma altura dado que la diferencia entre dos observaciones es la misma en ambos casos. Si por ejemplo la diferencia entre {5}, {6} es de 1, tanto con el enlace único como el completo, se fusionarían a la altura 1.

\
\
\
\

<h3 style="color:red">
Ejercicio 9: (12.6.10 ISL) página 551.
</h3>

10. In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.

(a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables. Hint: There are a number of functions in R that you can use to generate data. One example is the rnorm()function; runif() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes.

<h4 style="color:blue">  Solución </h4>

Extendiendo el ejemplo de la página 539 del *ISL*:

```{r}
classes = 3
n = 20
p = 50

set.seed(12)

x = matrix(0, 0, ncol=50, nrow=classes*n)

for(i in 1:n){
  x[i, ] = x[i, ]+rep(2, p)+rnorm(50, sd=0.5)
  x[i+20, ] = x[i+20, ]+rep(-2, p)+rnorm(50, sd=0.5)
  x[i+40, ] = x[i+40, ]+c( rep( 1, p/2 ), rep( -1, p/2 ) ) +rnorm(50, sd=0.5)
}

labels = c(rep(1, n), rep(2, n), rep(3, n))
```

(b) Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.

<h4 style="color:blue">  Solución </h4>

```{r, fig.width=6,fig.height=8,fig.cap="Fig. 22: PCA sobre las observaciones"}
pca.b <- prcomp(x, scale=TRUE)
plot(pca.b$x[, 1:2], col=labels, xlab="PC1", ylab="PC2")
```

\
(c) Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?

Hint: You can use the table() function in R to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same.

<h4 style="color:blue">  Solución </h4>

```{r}
set.seed(5)
kmeans.c <- kmeans(x, centers=3, nstart = 20)
table(labels, kmeans.c$cluster)
```
\
Los clústers se han construido de forma que cada uno contiene las 20 observaciones. Además, tal y como indica el enunciado, kmeans enumera arbitrariamente los clústers, por los que, al construir la tabla, podría darse el caso en que, los clústers estuvieran numerados asi:

2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Mientras que las etiquetas originales son así:

1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3

Es por ello que habría que tener cuidado a la hora de interpretar los resultados, ya que podrían diferir en cuanto a la numeración.

(d) Perform K-means clustering with K = 2. Describe your results.

```{r}
set.seed(5)
kmeans.d <- kmeans(x, centers=2, nstart = 20)
table(labels, kmeans.d$cluster)
```
\
Uno de los clusters ha desaparecido y por consecuencia, las observaciones pertenecientes a ese clúster desaparecido ahora pertenecen a uno de los restantes (el clúster 1).

(e) Now perform K-means clustering with K = 4, and describe your results.

```{r}
set.seed(5)
kmeans.e <- kmeans(x, centers=4, nstart = 20)
table(labels, kmeans.e$cluster)
```
\
Ahora, se ha creado otro clúster y la mitad de las observaciones de uno de los tres clústers (en concreto el número 2) han pasado al nuevo clúster (el número 4).


(f) Now perform K-means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K-means clustering on the 60×2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.

```{r}
set.seed(5)
kmeans.pca.ejf <- kmeans(pca.b$x[, 1:2], centers=3, nstart=20)
table(labels, kmeans.pca.ejf$cluster)
```
\
Se puede observar como se han creado los clústers con las respectivas 20 observaciones. Cabe puntualizar que kmeans enumera arbitrariamente los clústers, tal y como se puntualiza en el apartado (c).

(g) Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain.

```{r}
set.seed(5)
x.escalado <- scale(x)
kmeans.ej.g <- kmeans(x.escalado, centers=3, nstart=20)
table(labels, kmeans.ej.g$cluster)
```
A simple vista puede observarse como se han vuelto a crear los 3 clústers con sus respectivas 20 observaciones. Sin embargo, habría que llevar cuidado con el escalado, porque podría afectar a la distancia entre los puntos. 


\
\
\
\

<h3 style="color:red">
Ejercicio 1: (12.6.13 ISL) página 552.
</h3>

13. On the book website, www.statlearning.com, there is a gene expression data set (Ch12Ex13.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.

(a) Load in the data using read.csv(). You will need to select header = F.

```{r}
dataset <- read.csv(file = "./Ch12Ex13.csv", header=F)
head(dataset, 5)
```

(b) Apply hierarchical clustering to the samples using correlation-based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?

Con single linkage:

```{r, fig.cap="Figura 23. Single linkage con distancia basada en correlación."}
cluster.ejb <- hclust(as.dist(1-cor(dataset)), method="single")
plot(cluster.ejb, main="Single Linkage con distancia basada en correlación", xlab="", sub="")
```

Con complete linkage:

```{r, fig.cap="Figura 24. Complete linkage con distancia basada en correlación."}
cluster.ejb <- hclust(as.dist(1-cor(dataset)), method="complete")
plot(cluster.ejb, main="Complete Linkage con distancia basada en correlación", xlab="", sub="")
```

Con average linkage:

```{r, fig.cap="Figura 25. Average linkage con distancia basada en correlación."}
cluster.ejb <- hclust(as.dist(1-cor(dataset)), method="average")

plot(cluster.ejb, main="Average Linkage con distancia basada en correlación", xlab="", sub="")
```

Mediante el linkage simple y completo se han obtenido dos grupos, en cambio, con el linkage con "average" se han obtenido tres grupos. Por tanto, como ha podido visualizarse, los resultados dependen del tipo de linkage usado.

(c) Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here.

Esto se podría comprobar visualmente usando un gráfico jerárquico como el anterior y viendo la altura a la que se fusionan, sin embargo, sería inviable dado la cantidad de genes que, es decir, 1000 en total, saliendo un gráfico difícil de entender y en el que habrían muchos solapamientos. Se tiene que las primeras 20 columnas pertenecen a pacientes sanos, mientras que las 20 últimas pertenecen a pacientes enfermos. Por tanto, se tendrían 1000 variables y 40 muestras.

El primer paso, sería transponer el dataset para darle el formato anterior de 40 muestras con 1000 variables, y posteriormente escalar los datos:

```{r}
dataset.escalado = scale(t(dataset))
```

Se dividen los pacientes entre sanos y enfermos.

```{r}
sano <- dataset.escalado[1:20, ]
enfermo <- dataset.escalado[21:40, ]
```

A continuación se calcula la media de los pacientes sanos y de los pacientes enfermos.

```{r}
media.sanos <- apply(sano, 2, mean)
media.enfermo <- apply(enfermo, 2, mean)
```

Se calcula la matriz de diferencia de medias:

```{r}
mat <- matrix(abs(media.enfermo-media.sanos))
```

Se calculan los 8 genes que más difieren del resto:

```{r}
data.frame.genes <- as.data.frame(cbind(mat, 1:1000))[order(mat, decreasing=T),]

colnames(data.frame.genes) <- c("dist", "gen")

head(data.frame.genes, 8)
```

Los 8 genes que más difieren son 584, 593, 551, 565, 590, 600, 589 y 502.
